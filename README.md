
# ğŸŒŸ Framework de RÃ©seau de Neurones ğŸŒŸ

Bienvenue dans ce framework de rÃ©seau de neurones simple et efficace. Que vous soyez un dÃ©butant ou un expert en apprentissage automatique, ce framework vous permet de crÃ©er, entraÃ®ner et Ã©valuer des rÃ©seaux de neurones avec facilitÃ©.

## âœ¨ FonctionnalitÃ©s

- ğŸš€ **Multiples fonctions d'activation** : ReLU, Tanh, Sigmoid, Leaky ReLU
- ğŸ“‰ **Fonction de perte** : Erreur Quadratique Moyenne (MSE)
- ğŸ”— **Couches entiÃ¨rement connectÃ©es**
- ğŸ› ï¸ **Facile Ã  utiliser et Ã  Ã©tendre**

## ğŸš€ Installation

1. Clonez le dÃ©pÃ´t.
2. Assurez-vous d'avoir les packages requis installÃ©s :
    ```bash
    pip install numpy
    ```

## ğŸ“š Utilisation

### Exemple

Voici un exemple d'utilisation du framework avec un problÃ¨me simple de XOR :

```python
import numpy as np
from neural_network.neural_network import NeuralNetwork
from neural_network.activation_functions import ActivationFunction
from neural_network.loss_functions import mse, mse_prime

if __name__ == "__main__":
    # DonnÃ©es XOR
    x_train = np.array([
        [[0, 0]],
        [[0, 1]],
        [[1, 0]],
        [[1, 1]]
    ])
    y_train = np.array([[0.0], [1.0], [1.0], [0.0]])

    input_size = 2
    output_size = 1
    hidden_layers = 1
    neurons_per_layer = 2
    activation_function = ActivationFunction.LEAKY_RELU

    net = NeuralNetwork(input_size, output_size, hidden_layers, neurons_per_layer, activation_function, mse, mse_prime)
    net.fit(x_train, y_train, epochs=10000, learning_rate=0.01)

    out = net.predict(x_train)
    print(out)
```

### âš¡ Fonctions d'Activation

Le framework supporte les fonctions d'activation suivantes :

1. **ReLU (Rectified Linear Unit)**
    - **Sortie** : [0, +âˆ)
    - **DÃ©rivÃ©e** : 1 si x > 0, sinon 0
    - **Cas d'utilisation** : Capturer les non-linÃ©aritÃ©s, Ã©viter le problÃ¨me de gradient vanishing

2. **Tanh (Tangente Hyperbolique)**
    - **Sortie** : [-1, 1]
    - **DÃ©rivÃ©e** : 1 - tanh(x)^2
    - **Cas d'utilisation** : Normaliser les entrÃ©es entre [-1, 1], souvent utilisÃ© dans les couches cachÃ©es

3. **SigmoÃ¯de**
    - **Sortie** : (0, 1)
    - **DÃ©rivÃ©e** : Ïƒ(x) * (1 - Ïƒ(x)), oÃ¹ Ïƒ(x) est la fonction sigmoÃ¯de
    - **Cas d'utilisation** : Normaliser les sorties, souvent utilisÃ© pour des problÃ¨mes de classification binaire

4. **Leaky ReLU**
    - **Sortie** : (-âˆ, +âˆ) (mais principalement [0, +âˆ) avec une petite pente nÃ©gative)
    - **DÃ©rivÃ©e** : 1 si x > 0, sinon Î± (souvent Î± = 0.01)
    - **Cas d'utilisation** : Ã‰viter les neurones morts (problÃ¨me de gradient mort)

### ğŸ”§ ParamÃ¨tres

- **input_size** : Nombre de caractÃ©ristiques d'entrÃ©e
- **output_size** : Nombre de caractÃ©ristiques de sortie
- **hidden_layers** : Nombre de couches cachÃ©es
- **neurons_per_layer** : Nombre de neurones par couche cachÃ©e
- **activation_function** : Fonction d'activation Ã  utiliser (ReLU, Tanh, Sigmoid, Leaky ReLU)
- **loss_function** : Fonction de perte Ã  utiliser (actuellement seule la MSE est implÃ©mentÃ©e)
- **loss_function_prime** : DÃ©rivÃ©e de la fonction de perte
- **epochs** : Nombre d'itÃ©rations d'entraÃ®nement
- **learning_rate** : Taux d'apprentissage pour la mise Ã  jour des poids

### ğŸ¯ Obtenir de Bons RÃ©sultats

Pour obtenir de bons rÃ©sultats pour diffÃ©rents types de donnÃ©es, considÃ©rez les conseils suivants :

1. **ğŸ“Š Normalisation des DonnÃ©es** : Assurez-vous que vos donnÃ©es d'entrÃ©e sont normalisÃ©es. Par exemple, Ã©chellez vos caractÃ©ristiques pour avoir une moyenne nulle et une variance unitaire.
2. **âš™ï¸ Fonction d'Activation** : Choisissez la fonction d'activation qui convient le mieux Ã  votre problÃ¨me. Par exemple, utilisez Sigmoid ou Tanh pour les tÃ¢ches de classification binaire et ReLU pour les rÃ©seaux profonds.
3. **ğŸ—ï¸ Architecture du RÃ©seau** : ExpÃ©rimentez avec le nombre de couches et de neurones par couche. Les tÃ¢ches plus complexes peuvent nÃ©cessiter des rÃ©seaux plus profonds.
4. **ğŸ§  Taux d'Apprentissage** : Ajustez le taux d'apprentissage. Un taux trop Ã©levÃ© peut entraÃ®ner une convergence trop rapide vers une solution sous-optimale, tandis qu'un taux trop bas peut rendre le processus d'entraÃ®nement trop lent.
5. **â³ Ã‰poques** : EntraÃ®nez pendant un nombre suffisant d'Ã©poques. Cependant, mÃ©fiez-vous du surapprentissage ; envisagez d'utiliser l'arrÃªt prÃ©coce ou la validation croisÃ©e pour trouver le nombre optimal d'Ã©poques.

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ¤ Contribuer

Les contributions sont les bienvenues ! Veuillez ouvrir une issue ou soumettre une pull request pour toute amÃ©lioration ou correction de bug.

